
@article{lei_materials_2024,
	title = {Materials science in the era of large language models: a perspective},
	volume = {3},
	issn = {2635-098X},
	url = {https://pubs.rsc.org/en/content/articlelanding/2024/dd/d4dd00074a},
	doi = {10.1039/D4DD00074A},
	shorttitle = {Materials science in the era of large language models},
	abstract = {Large Language Models ({LLMs}) have garnered considerable interest due to their impressive natural language capabilities, which in conjunction with various emergent properties make them versatile tools in workflows ranging from complex code generation to heuristic finding for combinatorial problems. In this paper we offer a perspective on their applicability to materials science research, arguing their ability to handle ambiguous requirements across a range of tasks and disciplines means they could be a powerful tool to aid researchers. We qualitatively examine basic {LLM} theory, connecting it to relevant properties and techniques in the literature before providing two case studies that demonstrate their use in task automation and knowledge extraction at-scale. At their current stage of development, we argue {LLMs} should be viewed less as oracles of novel insight, and more as tireless workers that can accelerate and unify exploration across domains. It is our hope that this paper can familiarise materials science researchers with the concepts needed to leverage these tools in their own research.},
	pages = {1257--1272},
	number = {7},
	journaltitle = {Digital Discovery},
	shortjournal = {Digital Discovery},
	author = {Lei, Ge and Docherty, Ronan and Cooper, Samuel J.},
	urldate = {2024-09-11},
	date = {2024-07-10},
	langid = {english},
	note = {Publisher: {RSC}},
	keywords = {Para Leitura},
	file = {Full Text PDF:/home/breno/Zotero/storage/QIQWDHCC/Lei et al. - 2024 - Materials science in the era of large language mod.pdf:application/pdf;Supplementary Information PDF:/home/breno/Zotero/storage/SJARUAJF/Lei et al. - 2024 - Materials science in the era of large language mod.pdf:application/pdf},
}

@article{katsura_natural_2024,
	title = {Natural Language Processing for Materials Informatics of Literature Data: —Examples in the Superconducting Material Development—},
	volume = {144},
	issn = {0385-4205, 1347-5533},
	url = {https://www.jstage.jst.go.jp/article/ieejfms/144/9/144_350/_article/-char/ja/},
	doi = {10.1541/ieejfms.144.350},
	shorttitle = {Natural Language Processing for Materials Informatics of Literature Data},
	pages = {350--359},
	number = {9},
	journaltitle = {{IEEJ} Transactions on Fundamentals and Materials},
	shortjournal = {The transactions of the Institute of Electrical Engineers of Japan.A},
	author = {Katsura, Yukari},
	urldate = {2024-09-11},
	date = {2024-09-01},
	langid = {english},
}

@misc{wu_next-gpt_2024,
	title = {{NExT}-{GPT}: Any-to-Any Multimodal {LLM}},
	url = {http://arxiv.org/abs/2309.05519},
	doi = {10.48550/arXiv.2309.05519},
	shorttitle = {{NExT}-{GPT}},
	abstract = {While recently Multimodal Large Language Models ({MM}-{LLMs}) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any {MM}-{LLMs} capable of accepting and delivering content in any modality becomes essential to human-level {AI}. To fill the gap, we present an end-to-end general-purpose any-to-any {MM}-{LLM} system, {NExT}-{GPT}. We connect an {LLM} with multimodal adaptors and different diffusion decoders, enabling {NExT}-{GPT} to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, {NExT}-{GPT} is tuned with only a small amount of parameter (1\%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning ({MosIT}) and manually curate a high-quality dataset for {MosIT}, based on which {NExT}-{GPT} is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an {AI} agent capable of modeling universal modalities, paving the way for more human-like {AI} research in the community. Project page: https://next-gpt.github.io/},
	number = {{arXiv}:2309.05519},
	publisher = {{arXiv}},
	author = {Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng},
	urldate = {2024-09-03},
	date = {2024-06-25},
	eprinttype = {arxiv},
	eprint = {2309.05519 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/ZIIVLMCV/Wu et al. - 2024 - NExT-GPT Any-to-Any Multimodal LLM.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/ZV43BWDE/2309.html:text/html},
}

@online{dubey_llama_2024,
	title = {The Llama 3 Herd of Models},
	url = {https://arxiv.org/abs/2407.21783v2},
	abstract = {Modern artificial intelligence ({AI}) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as {GPT}-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	titleaddon = {{arXiv}.org},
	author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and {McConnell}, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and {AlBadawy}, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and van der Linde, Jelmer and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Rantala-Yeary, Lauren and van der Maaten, Laurens and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and de Oliveira, Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Tan, Xiaoqing Ellen and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Grattafiori, Aaron and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Vaughan, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Franco, Annie and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and De Paola, Beto and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Wyatt, Danny and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Ozgenel, Firat and Caggioni, Francesco and Guzmán, Francisco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Thattai, Govind and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and {McPhie}, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Laptev, Nikolay Pavlovich and Dong, Ning and Zhang, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Maheswari, Rohan and Howes, Russ and Rinott, Ruty and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Albiero, Vítor and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang, Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Hao, Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and {DeVito}, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei},
	urldate = {2024-08-29},
	date = {2024-07-31},
	langid = {english},
	file = {Full Text PDF:/home/breno/Zotero/storage/HNY3SEXB/Dubey et al. - 2024 - The Llama 3 Herd of Models.pdf:application/pdf},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 Technical Report},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of {GPT}-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, {GPT}-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. {GPT}-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of {GPT}-4's performance based on models trained with no more than 1/1,000th the compute of {GPT}-4.},
	number = {{arXiv}:2303.08774},
	publisher = {{arXiv}},
	author = {{OpenAI} and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and {McGrew}, Bob and {McKinney}, Scott Mayer and {McLeavey}, Christine and {McMillan}, Paul and {McNeil}, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	urldate = {2024-08-29},
	date = {2024-03-04},
	eprinttype = {arxiv},
	eprint = {2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Para Leitura},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/MIHS62N4/OpenAI et al. - 2024 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/THFMBDPX/2303.html:text/html},
}

@misc{brown_language_2020,
	title = {Language Models are Few-Shot Learners},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we find that {GPT}-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of {GPT}-3 in general.},
	number = {{arXiv}:2005.14165},
	publisher = {{arXiv}},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2024-08-29},
	date = {2020-07-22},
	eprinttype = {arxiv},
	eprint = {2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language, Para Leitura},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/HCZPBEYI/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/99FYK52W/2005.html:text/html},
}

@article{lee_biobert_2020,
	title = {{BioBERT}: A pre-trained biomedical language representation model for biomedical text mining},
	volume = {36},
	doi = {10.1093/bioinformatics/btz682},
	shorttitle = {{BioBERT}},
	abstract = {Motivation: Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing ({NLP}), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in {NLP} to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model {BERT} can be adapted for biomedical corpora. Results: We introduce {BioBERT} (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, {BioBERT} largely outperforms {BERT} and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While {BERT} obtains performance comparable to that of previous state-of-the-art models, {BioBERT} significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62\% F1 score improvement), biomedical relation extraction (2.80\% F1 score improvement) and biomedical question answering (12.24\% {MRR} improvement). Our analysis results show that pre-training {BERT} on biomedical corpora helps it to understand complex biomedical texts. © 2020 Oxford University Press. All rights reserved.},
	pages = {1234--1240},
	number = {4},
	journaltitle = {Bioinformatics},
	author = {Lee, J. and Yoon, W. and Kim, S. and Kim, D. and Kim, S. and So, C.H. and Kang, J.},
	date = {2020},
	keywords = {Para Leitura},
	file = {Snapshot:/home/breno/Zotero/storage/9A3YLXPI/display.html:text/html;Texto completo:/home/breno/Zotero/storage/PRHHV7R7/Lee et al. - 2020 - BioBERT A pre-trained biomedical language represe.pdf:application/pdf},
}

@inproceedings{wolf_transformers_2020,
	location = {Online},
	title = {Transformers: State-of-the-Art Natural Language Processing},
	url = {https://aclanthology.org/2020.emnlp-demos.6},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	shorttitle = {Transformers},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified {API}. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	pages = {38--45},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	editor = {Liu, Qun and Schlangen, David},
	urldate = {2024-08-29},
	date = {2020-10},
	file = {Full Text PDF:/home/breno/Zotero/storage/QV4SFA7L/Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Pr.pdf:application/pdf},
}

@inproceedings{wolf_transformers_2020-1,
	title = {Transformers: State-of-the-Art Natural Language Processing},
	shorttitle = {Transformers},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered stateof- the art Transformer architectures under a unified {API}. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers. © 2020 Association for Computational Linguistics.},
	eventtitle = {{EMNLP} 2020 - Conference on Empirical Methods in Natural Language Processing, Proceedings of Systems Demonstrations},
	pages = {38--45},
	author = {Wolf, T. and Debut, L. and Sanh, V. and Chaumond, J. and Delangue, C. and Moi, A. and Cistac, P. and Rault, T. and Louf, R. and Funtowicz, M. and Davison, J. and Shleifer, S. and Von Platen, P. and Ma, C. and Jernite, Y. and Plu, J. and Xu, C. and Le Scao, T. and Gugger, S. and Drame, M. and Lhoest, Q. and Rush, A.M.},
	date = {2020},
	file = {Snapshot:/home/breno/Zotero/storage/R6ZE2BP9/display.html:text/html},
}

@misc{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {{arXiv}:1810.04805},
	publisher = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2024-08-28},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language, Para Leitura},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/TMYL7TGL/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/VVA6HRUV/1810.html:text/html},
}

@article{roy_scispace_2024,
	title = {{SciSpace} Copilot: Empowering Researchers through Intelligent Reading Assistance},
	volume = {38},
	issn = {23743468},
	url = {https://www.mendeley.com/catalogue/02d8cfb7-e039-341d-9582-56f4624feba1/},
	doi = {10.1609/aaai.v38i21.30578},
	shorttitle = {{SciSpace} Copilot},
	abstract = {(2024) Roy et al. Proceedings of the {AAAI} Conference on Artificial Intelligence. We introduce {SciSpace} Copilot, an {AI} research assistant that helps in understanding and reading research papers fast...},
	pages = {23826--23828},
	number = {21},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Roy, Trinita and Kumar, Asheesh and Raghuvanshi, Daksh and Jain, Siddhant and Vignesh, Goutham and Shinde, Kartik and Tondulkar, Rohan},
	urldate = {2024-08-24},
	date = {2024},
	langid = {british},
	note = {Number: 21},
	keywords = {Lido},
	file = {Snapshot:/home/breno/Zotero/storage/PVMHDWY3/02d8cfb7-e039-341d-9582-56f4624feba1.html:text/html;Texto completo:/home/breno/Zotero/storage/FUM823XN/Roy et al. - 2024 - SciSpace Copilot Empowering Researchers through I.pdf:application/pdf},
}

@misc{li_survey_2022,
	title = {A Survey on Retrieval-Augmented Text Generation},
	url = {http://arxiv.org/abs/2202.01110},
	doi = {10.48550/arXiv.2202.01110},
	abstract = {Recently, retrieval-augmented text generation attracted increasing attention of the computational linguistics community. Compared with conventional generation models, retrieval-augmented text generation has remarkable advantages and particularly has achieved state-of-the-art performance in many {NLP} tasks. This paper aims to conduct a survey about retrieval-augmented text generation. It firstly highlights the generic paradigm of retrieval-augmented generation, and then it reviews notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks. Finally, it points out some important directions on top of recent methods to facilitate future research.},
	number = {{arXiv}:2202.01110},
	publisher = {{arXiv}},
	author = {Li, Huayang and Su, Yixuan and Cai, Deng and Wang, Yan and Liu, Lemao},
Retrieval
	eprint = {2202.01110 [cs]},
	keywords = {Computer Science - Computation and Language, Lido},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/9HREZSRF/Li et al. - 2022 - A Survey on Retrieval-Augmented Text Generation.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/T5QVDJFB/2202.html:text/html},
}

@misc{santhanam_colbertv2_2022,
	title = {{ColBERTv}2: Effective and Efficient Retrieval via Lightweight Late Interaction},
	url = {http://arxiv.org/abs/2112.01488},
	doi = {10.48550/arXiv.2112.01488},
	shorttitle = {{ColBERTv}2},
	abstract = {Neural information retrieval ({IR}) has greatly advanced search and other knowledge-intensive language tasks. While many neural {IR} methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce {ColBERTv}2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate {ColBERTv}2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10\${\textbackslash}times\$.},
	number = {{arXiv}:2112.01488},Retrieval
	publisher = {{arXiv}},
	author = {Santhanam, Keshav and Khattab, Omar and Saad-Falcon, Jon and Potts, Christopher and Zaharia, Matei},
	urldate = {2024-08-24},
	date = {2022-07-10},
	eprinttype = {arxiv},
	eprint = {2112.01488 [cs]},
	keywords = {Computer Science - Computation and Language, Para Leitura, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/IWA6BI8W/Santhanam et al. - 2022 - ColBERTv2 Effective and Efficient Retrieval via L.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/SMGRF2XR/2112.html:text/html},
}

@inproceedings{ter_hoeve_conversations_2020,
	title = {Conversations with Documents. An Exploration of Document-Centered Assistance},
	url = {http://arxiv.org/abs/2002.00747},
	doi = {10.1145/3343413.3377971},
	abstract = {The role of conversational assistants has become more prevalent in helping people increase their productivity. Document-centered assistance, for example to help an individual quickly review a document, has seen less significant progress, even though it has the potential to tremendously increase a user's productivity. This type of document-centered assistance is the focus of this paper. Our contributions are three-fold: (1) We first present a survey to understand the space of document-centered assistance and the capabilities people expect in this scenario. (2) We investigate the types of queries that users will pose while seeking assistance with documents, and show that document-centered questions form the majority of these queries. (3) We present a set of initial machine learned models that show that (a) we can accurately detect document-centered questions, and (b) we can build reasonably accurate models for answering such questions. These positive results are encouraging, and suggest that even greater results may be attained with continued study of this interesting and novel problem space. Our findings have implications for the design of intelligent systems to support task completion via natural interactions with documents.},
	pages = {43--52},
	booktitle = {Proceedings of the 2020 Conference on Human Information Interaction and Retrieval},
	author = {ter Hoeve, Maartje and Sim, Robert and Nouri, Elnaz and Fourney, Adam and de Rijke, Maarten and White, Ryen W.},
	urldate = {2024-08-21},
	date = {2020-03-14},
	eprinttype = {arxiv},
	eprint = {2002.00747 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Para Leitura, Computer Science - Information Retrieval, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/VZCK5FXA/ter Hoeve et al. - 2020 - Conversations with Documents. An Exploration of Do.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/F2WSE9N5/2002.html:text/html},
}

@article{panda_enhancing_nodate,
	title = {Enhancing {PDF} interaction for a more engaging user experience in library: Introducing {ChatPDF}},
	volume = {8},
	issn = {2456-9623},
	url = {https://www.ijlsit.org/article-details/19094},
	doi = {10.18231/j.ijlsit.2023.004},
	shorttitle = {Enhancing {PDF} interaction for a more engaging user experience in library},
	abstract = {Enhancing {PDF} interaction for a more engaging user experience in library: Introducing {ChatPDF} - {IJLSIT}- Print {ISSN} No: - 2582-1555 Online {ISSN} No:- 2456-9623 Article {DOI} No:- 10.18231/j.ijlsit.2023.004, {IP} Indian Journal of Library Science and Information Technology-{IP} Indian J Libr Sci Inf Technol},
	pages = {20--25},
	number = {1},
	journaltitle = {{IP} Indian Journal of Library Science and Information Technology},
	author = {Panda, Subhajit},
	urldate = {2024-08-23},
	langid = {english},
	keywords = {Lido},
	file = {Full Text PDF:/home/breno/Zotero/storage/V9GMMUNG/Panda - Enhancing PDF interaction for a more engaging user.pdf:application/pdf},
}

@inproceedings{ter_hoeve_conversations_2020-1,
	title = {Conversations with Documents. An Exploration of Document-Centered Assistance},
	url = {http://arxiv.org/abs/2002.00747},
	doi = {10.1145/3343413.3377971},
	abstract = {The role of conversational assistants has become more prevalent in helping people increase their productivity. Document-centered assistance, for example to help an individual quickly review a document, has seen less significant progress, even though it has the potential to tremendously increase a user's productivity. This type of document-centered assistance is the focus of this paper. Our contributions are three-fold: (1) We first present a survey to understand the space of document-centered assistance and the capabilities people expect in this scenario. (2) We investigate the types of queries that users will pose while seeking assistance with documents, and show that document-centered questions form the majority of these queries. (3) We present a set of initial machine learned models that show that (a) we can accurately detect document-centered questions, and (b) we can build reasonably accurate models for answering such questions. These positive results are encouraging, and suggest that even greater results may be attained with continued study of this interesting and novel problem space. Our findings have implications for the design of intelligent systems to support task completion via natural interactions with documents.},
	pages = {43--52},
	booktitle = {Proceedings of the 2020 Conference on Human Information Interaction and Retrieval},
	author = {ter Hoeve, Maartje and Sim, Robert and Nouri, Elnaz and Fourney, Adam and de Rijke, Maarten and White, Ryen W.},
	urldate = {2024-08-21},
	date = {2020-03-14},
	eprinttype = {arxiv},
	eprint = {2002.00747 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/TJ9WLSHL/ter Hoeve et al. - 2020 - Conversations with Documents. An Exploration of Do.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/96FVV93N/2002.html:text/html},
}

@article{venugopal_looking_2021,
	title = {Looking through glass: Knowledge discovery from materials science literature using natural language processing},
	volume = {2},
	issn = {2666-3899},
	url = {https://www.sciencedirect.com/science/article/pii/S2666389921001239},
	doi = {10.1016/j.patter.2021.100290},
	shorttitle = {Looking through glass},
	abstract = {Most of the knowledge in materials science literature is in the form of unstructured data such as text and images. Here, we present a framework employing natural language processing, which automates text and image comprehension and precision knowledge extraction from inorganic glasses’ literature. The abstracts are automatically categorized using latent Dirichlet allocation ({LDA}) to classify and search semantically linked publications. Similarly, a comprehensive summary of images and plots is presented using the caption cluster plot ({CCP}), providing direct access to images buried in the papers. Finally, we combine the {LDA} and {CCP} with chemical elements to present an elemental map, a topical and image-wise distribution of elements occurring in the literature. Overall, the framework presented here can be a generic and powerful tool to extract and disseminate material-specific information on composition–structure–processing–property dataspaces, allowing insights into fundamental problems relevant to the materials science community and accelerated materials discovery.},
	pages = {100290},
	number = {7},
	journaltitle = {Patterns},
	shortjournal = {Patterns},
	author = {Venugopal, Vineeth and Sahoo, Sourav and Zaki, Mohd and Agarwal, Manish and Gosvami, Nitya Nand and Krishnan, N. M. Anoop},
	urldate = {2024-01-28},
	date = {2021-07-09},
	keywords = {Lido, artificial intelligence, glass science, knowledge discovery, materials science, natural language processing},
	file = {ScienceDirect Snapshot:/home/breno/Zotero/storage/YQPIDX4U/S2666389921001239.html:text/html;Texto completo:/home/breno/Zotero/storage/EPH8UUHR/Venugopal et al. - 2021 - Looking through glass Knowledge discovery from ma.pdf:application/pdf},
}

@inproceedings{lewis_retrieval-augmented_2020,
	title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream {NLP} tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation ({RAG}) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce {RAG} models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two {RAG} formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive {NLP} tasks and set the state-of-the-art on three open domain {QA} tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that {RAG} models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	pages = {9459--9474},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	urldate = {2024-07-26},
	date = {2020},
	keywords = {Lido},
	file = {Full Text PDF:/home/breno/Zotero/storage/DHI7B2LJ/Lewis et al. - 2020 - Retrieval-Augmented Generation for Knowledge-Inten.pdf:application/pdf},
}

@misc{wang_interactive_2022,
	title = {Interactive Data Analysis with Next-step Natural Language Query Recommendation},
	url = {http://arxiv.org/abs/2201.04868},
	abstract = {Natural language interfaces ({NLIs}) provide users with a convenient way to interactively analyze data through natural language queries. Nevertheless, interactive data analysis is a demanding process, especially for novice data analysts. When exploring large and complex {SQL} databases from different domains, data analysts do not necessarily have sufficient knowledge about different data tables and application domains. It makes them unable to systematically elicit a series of topically-related and meaningful queries for insight discovery in target domains. We develop a {NLI} with a step-wise query recommendation module to assist users in choosing appropriate next-step exploration actions. The system adopts a data-driven approach to suggest semantically relevant and context-aware queries for application domains of users' interest based on their query logs. Also, the system helps users organize query histories and results into a dashboard to communicate the discovered data insights. With a comparative user study, we show that our system can facilitate a more effective and systematic data analysis process than a baseline without the recommendation module.},
	number = {{arXiv}:2201.04868},
	publisher = {{arXiv}},
	author = {Wang, Xingbo and Cheng, Furui and Wang, Yong and Xu, Ke and Long, Jiang and Lu, Hong and Qu, Huamin},
	urldate = {2024-07-26},
	date = {2022-11-01},
	eprinttype = {arxiv},
	eprint = {2201.04868 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Lido, Computer Science - Databases},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/JQWTSTSH/Wang et al. - 2022 - Interactive Data Analysis with Next-step Natural L.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/EEWVB9EY/2201.html:text/html},
}

@misc{touvron_llama_2023,
	title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
	url = {http://arxiv.org/abs/2307.09288},
	shorttitle = {Llama 2},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models ({LLMs}) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned {LLMs}, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of {LLMs}.},
	number = {{arXiv}:2307.09288},
	publisher = {{arXiv}},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	urldate = {2024-07-26},
	date = {2023-07-19},
	eprinttype = {arxiv},
	eprint = {2307.09288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Para Leitura},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/7LJZIYGT/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/ZTPLP854/2307.html:text/html},
}

@misc{august_paper_2022,
	title = {Paper Plain: Making Medical Research Papers Approachable to Healthcare Consumers with Natural Language Processing},
	url = {http://arxiv.org/abs/2203.00130},
	doi = {10.48550/arXiv.2203.00130},
	shorttitle = {Paper Plain},
	abstract = {When seeking information not covered in patient-friendly documents, like medical pamphlets, healthcare consumers may turn to the research literature. Reading medical papers, however, can be a challenging experience. To improve access to medical papers, we introduce a novel interactive interface-Paper Plain-with four features powered by natural language processing: definitions of unfamiliar terms, in-situ plain language section summaries, a collection of key questions that guide readers to answering passages, and plain language summaries of the answering passages. We evaluate Paper Plain, finding that participants who use Paper Plain have an easier time reading and understanding research papers without a loss in paper comprehension compared to those who use a typical {PDF} reader. Altogether, the study results suggest that guiding readers to relevant passages and providing plain language summaries, or "gists," alongside the original paper content can make reading medical papers easier and give readers more confidence to approach these papers.},
	number = {{arXiv}:2203.00130},
	publisher = {{arXiv}},
	author = {August, Tal and Wang, Lucy Lu and Bragg, Jonathan and Hearst, Marti A. and Head, Andrew and Lo, Kyle},
	urldate = {2024-07-26},
	date = {2022-02-28},
	eprinttype = {arxiv},
	eprint = {2203.00130 [cs]},
	keywords = {Computer Science - Computation and Language, Para Leitura, Computer Science - Human-Computer Interaction, H.5.2},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/A2NLV8VV/August et al. - 2022 - Paper Plain Making Medical Research Papers Approa.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/4YXUYWA5/2203.html:text/html},
}

@inproceedings{lo_papermage_2023,
	location = {Singapore},
	title = {{PaperMage}: A Unified Toolkit for Processing, Representing, and Manipulating Visually-Rich Scientific Documents},
	url = {https://aclanthology.org/2023.emnlp-demo.45},
	doi = {10.18653/v1/2023.emnlp-demo.45},
	shorttitle = {{PaperMage}},
	abstract = {Despite growing interest in applying natural language processing ({NLP}) and computer vision ({CV}) models to the scholarly domain, scientific documents remain challenging to work with. They're often in difficult-to-use {PDF} formats, and the ecosystem of models to process them is fragmented and incomplete. We introduce {PaperMage}, an open-source Python toolkit for analyzing and processing visually-rich, structured scientific documents. {PaperMage} offers clean and intuitive abstractions for seamlessly representing and manipulating both textual and visual document elements. {PaperMage} achieves this by integrating disparate state-of-the-art {NLP} and {CV} models into a unified framework, and provides turn-key recipes for common scientific document processing use-cases. {PaperMage} has powered multiple research prototypes of {AI} applications over scientific documents, along with Semantic Scholar's large-scale production system for processing millions of {PDFs}. {GitHub}: https://github.com/allenai/papermage},
	pages = {495--507},
	booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
	publisher = {Association for Computational Linguistics},
	author = {Lo, Kyle and Shen, Zejiang and Newman, Benjamin and Chang, Joseph and Authur, Russell and Bransom, Erin and Candra, Stefan and Chandrasekhar, Yoganand and Huff, Regan and Kuehl, Bailey and Singh, Amanpreet and Wilhelm, Chris and Zamarron, Angele and Hearst, Marti A. and Weld, Daniel and Downey, Doug and Soldaini, Luca},
	editor = {Feng, Yansong and Lefever, Els},
	urldate = {2024-07-26},
	date = {2023-12},
	keywords = {Para Leitura},
	file = {Full Text PDF:/home/breno/Zotero/storage/9YNWQ5SX/Lo et al. - 2023 - PaperMage A Unified Toolkit for Processing, Repre.pdf:application/pdf},
}

@inproceedings{zeng_socratic_2022,
	title = {Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
	url = {https://openreview.net/forum?id=G2Q2Mh3avow},
	shorttitle = {Socratic Models},
	abstract = {We investigate how multimodal prompt engineering can use language as the intermediate representation to combine complementary knowledge from different pretrained (potentially multimodal) language models for a variety of tasks. This approach is both distinct from and complementary to the dominant paradigm of joint multimodal training. It also recalls a traditional systems-building view as in classical {NLP} pipelines, but with prompting large pretrained multimodal models. We refer to these as Socratic Models ({SMs}): a modular class of systems in which multiple pretrained models may be composed zero-shot via multimodal-informed prompting to capture new multimodal capabilities, without additional finetuning. We show that these systems provide competitive state-of-the-art performance for zero-shot image captioning and video-to-text retrieval, and also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes), and (iii) robot perception and planning. We hope this work provides (a) results for stronger zero-shot baseline performance with analysis also highlighting their limitations, (b) new perspectives for building multimodal systems powered by large pretrained models, and (c) practical application advantages in certain regimes limited by data scarcity, training compute, or model access.},
	eventtitle = {The Eleventh International Conference on Learning Representations},
	author = {Zeng, Andy and Attarian, Maria and Ichter, Brian and Choromanski, Krzysztof Marcin and Wong, Adrian and Welker, Stefan and Tombari, Federico and Purohit, Aveek and Ryoo, Michael S. and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and Florence, Pete},
	urldate = {2024-07-26},
	date = {2022-09-29},
	langid = {english},
	keywords = {Para Leitura},
	file = {Full Text PDF:/home/breno/Zotero/storage/8QJPAIQJ/Zeng et al. - 2022 - Socratic Models Composing Zero-Shot Multimodal Re.pdf:application/pdf},
}

@article{dagdelen_structured_2024,
	title = {Structured information extraction from scientific text with large language models},
	volume = {15},
	rights = {2024 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-45563-x},
	doi = {10.1038/s41467-024-45563-x},
	abstract = {Extracting structured knowledge from scientific text remains a challenging task for machine learning models. Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pretrained large language models ({GPT}-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of {JSON} objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.},
	pages = {1418},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Dagdelen, John and Dunn, Alexander and Lee, Sanghoon and Walker, Nicholas and Rosen, Andrew S. and Ceder, Gerbrand and Persson, Kristin A. and Jain, Anubhav},
	urldate = {2024-07-26},
	date = {2024-02-15},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Para Leitura, Databases, Materials science, Scientific data, Theory and computation},
	file = {Full Text PDF:/home/breno/Zotero/storage/ER7W5FXC/Dagdelen et al. - 2024 - Structured information extraction from scientific .pdf:application/pdf},
}

@article{ji_survey_2023,
	title = {Survey of Hallucination in Natural Language Generation},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	url = {http://arxiv.org/abs/2202.03629},
	doi = {10.1145/3571730},
	abstract = {Natural Language Generation ({NLG}) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent {NLG}, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in {NLG}. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation, and visual-language generation; and (3) hallucinations in large language models ({LLMs}). This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in {NLG}.},
	pages = {1--38},
	number = {12},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Chen, Delong and Dai, Wenliang and Chan, Ho Shu and Madotto, Andrea and Fung, Pascale},
	urldate = {2024-07-26},
	date = {2023-12-31},
	eprinttype = {arxiv},
	eprint = {2202.03629 [cs]},
	keywords = {Computer Science - Computation and Language, Para Leitura, A.1},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/X7HFF3R6/Ji et al. - 2023 - Survey of Hallucination in Natural Language Genera.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/EKYLUI9L/2202.html:text/html},
}

@misc{menick_teaching_2022,
	title = {Teaching language models to support answers with verified quotes},
	url = {http://arxiv.org/abs/2203.11147},
	abstract = {Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences ({RLHP}) to train "open-book" {QA} models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, {GopherCite}, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of {GopherCite} by conducting human evaluation of answers to questions in a subset of the {NaturalQuestions} and {ELI}5 datasets. The model's response is found to be high-quality 80{\textbackslash}\% of the time on this Natural Questions subset, and 67{\textbackslash}\% of the time on the {ELI}5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90{\textbackslash}\% and 80{\textbackslash}\% respectively, approaching human baselines. However, analysis on the adversarial {TruthfulQA} dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.},
	number = {{arXiv}:2203.11147},
	publisher = {{arXiv}},
	author = {Menick, Jacob and Trebacz, Maja and Mikulik, Vladimir and Aslanides, John and Song, Francis and Chadwick, Martin and Glaese, Mia and Young, Susannah and Campbell-Gillingham, Lucy and Irving, Geoffrey and {McAleese}, Nat},
	urldate = {2024-07-26},
	date = {2022-03-21},
	eprinttype = {arxiv},
	eprint = {2203.11147 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Para Leitura},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/CG3HC2JE/Menick et al. - 2022 - Teaching language models to support answers with v.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/5P2HTDMA/2203.html:text/html},
}

@article{tkaczyk_cermine_2015,
	title = {{CERMINE}: automatic extraction of structured metadata from scientific literature},
	volume = {18},
	issn = {1433-2825},
	url = {https://doi.org/10.1007/s10032-015-0249-8},
	doi = {10.1007/s10032-015-0249-8},
	shorttitle = {{CERMINE}},
	abstract = {{CERMINE} is a comprehensive open-source system for extracting structured metadata from scientific articles in a born-digital form. The system is based on a modular workflow, whose loosely coupled architecture allows for individual component evaluation and adjustment, enables effortless improvements and replacements of independent parts of the algorithm and facilitates future architecture expanding. The implementations of most steps are based on supervised and unsupervised machine learning techniques, which simplifies the procedure of adapting the system to new document layouts and styles. The evaluation of the extraction workflow carried out with the use of a large dataset showed good performance for most metadata types, with the average F score of 77.5 \%. {CERMINE} system is available under an open-source licence and can be accessed at http://cermine.ceon.pl. In this paper, we outline the overall workflow architecture and provide details about individual steps implementations. We also thoroughly compare {CERMINE} to similar solutions, describe evaluation methodology and finally report its results.},
	pages = {317--335},
	number = {4},
	journaltitle = {International Journal on Document Analysis and Recognition ({IJDAR})},
	shortjournal = {{IJDAR}},
	author = {Tkaczyk, Dominika and Szostek, Paweł and Fedoryszak, Mateusz and Dendek, Piotr Jan and Bolikowski, Łukasz},
	urldate = {2024-07-26},
	date = {2015-12-01},
	langid = {english},
	keywords = {Para Leitura, Bibliography extraction, Content classification, Metadata extraction, Reference parsing, Scientific literature analysis},
	file = {Full Text PDF:/home/breno/Zotero/storage/S8WRKG8X/Tkaczyk et al. - 2015 - CERMINE automatic extraction of structured metada.pdf:application/pdf},
}

@article{caillou_cartolabe_2021,
	title = {Cartolabe: A Web-Based Scalable Visualization of Large Document Collections},
	volume = {41},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0272-1716, 1558-1756},
	url = {https://ieeexplore.ieee.org/document/9238399/},
	doi = {10.1109/MCG.2020.3033401},
	shorttitle = {Cartolabe},
	pages = {76--88},
	number = {2},
	journaltitle = {{IEEE} Computer Graphics and Applications},
	shortjournal = {{IEEE} Comput. Grap. Appl.},
	author = {Caillou, Philippe and Renault, Jonas and Fekete, Jean-Daniel and Letournel, Anne-Catherine and Sebag, Michele},
	urldate = {2024-07-15},
	date = {2021-03-01},
	keywords = {Para Leitura},
	file = {Versão submetida:/home/breno/Zotero/storage/C4DH7Y3E/Caillou et al. - 2021 - Cartolabe A Web-Based Scalable Visualization of L.pdf:application/pdf},
}

@misc{wang_scidasynth_2024,
	title = {{SciDaSynth}: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model},
	url = {http://arxiv.org/abs/2404.13765},
	shorttitle = {{SciDaSynth}},
	abstract = {Extraction and synthesis of structured knowledge from extensive scientific literature are crucial for advancing and disseminating scientific progress. Although many existing systems facilitate literature review and digest, they struggle to process multimodal, varied, and inconsistent information within and across the literature into structured data. We introduce {SciDaSynth}, a novel interactive system powered by large language models ({LLMs}) that enables researchers to efficiently build structured knowledge bases from scientific literature at scale. The system automatically creates data tables to organize and summarize users' interested knowledge in literature via question-answering. Furthermore, it provides multi-level and multi-faceted exploration of the generated data tables, facilitating iterative validation, correction, and refinement. Our within-subjects study with researchers demonstrates the effectiveness and efficiency of {SciDaSynth} in constructing quality scientific knowledge bases. We further discuss the design implications for human-{AI} interaction tools for data extraction and structuring.},
	number = {{arXiv}:2404.13765},
	publisher = {{arXiv}},
	author = {Wang, Xingbo and Huey, Samantha L. and Sheng, Rui and Mehta, Saurabh and Wang, Fei},
	urldate = {2024-07-14},
	date = {2024-04-21},
	eprinttype = {arxiv},
	eprint = {2404.13765 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Lido},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/EL8UXS4H/Wang et al. - 2024 - SciDaSynth Interactive Structured Knowledge Extra.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/A6K376GA/2404.html:text/html},
}

@article{petersen_guidelines_2015,
	title = {Guidelines for conducting systematic mapping studies in software engineering: An update},
	volume = {64},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584915000646},
	doi = {10.1016/j.infsof.2015.03.007},
	shorttitle = {Guidelines for conducting systematic mapping studies in software engineering},
	abstract = {Context
Systematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews ({SLRs}). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and {SLR} guidelines.
Objective
To identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.); to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly.
Method
We conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to defining the search and to conduct a quality assessment).
Results
In a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given.
Conclusion
The most frequently followed guidelines are not sufficient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing findings.},
	pages = {1--18},
	journaltitle = {Information and Software Technology},
	shortjournal = {Information and Software Technology},
	author = {Petersen, Kai and Vakkalanka, Sairam and Kuzniarz, Ludwik},
	urldate = {2024-03-11},
	date = {2015-08-01},
	keywords = {Lido, Guidelines, Software engineering, Systematic mapping studies},
	file = {Petersen et al. - 2015 - Guidelines for conducting systematic mapping studi.pdf:/home/breno/Zotero/storage/22TCPG8C/Petersen et al. - 2015 - Guidelines for conducting systematic mapping studi.pdf:application/pdf;ScienceDirect Snapshot:/home/breno/Zotero/storage/36KJ686I/S0950584915000646.html:text/html},
}

@misc{polak_extracting_2023,
	title = {Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering},
	url = {http://arxiv.org/abs/2303.05352},
	abstract = {There has been a growing effort to replace hand extraction of data from research papers with automated data extraction based on natural language processing, language models, and recently, large language models ({LLMs}). Although these methods enable efficient extraction of data from large sets of research papers, they require a significant amount of up-front effort, expertise, and coding. In this work we propose the {ChatExtract} method that can fully automate very accurate data extraction with minimal initial effort and background, using an advanced conversational {LLM}. {ChatExtract} consists of a set of engineered prompts applied to a conversational {LLM} that both identify sentences with data, extract that data, and assure the data's correctness through a series of follow-up questions. These follow-up questions largely overcome known issues with {LLMs} providing factually inaccurate responses. {ChatExtract} can be applied with any conversational {LLMs} and yields very high quality data extraction. In tests on materials data we find precision and recall both close to 90\% from the best conversational {LLMs}, like {ChatGPT}-4. We demonstrate that the exceptional performance is enabled by the information retention in a conversational model combined with purposeful redundancy and introducing uncertainty through follow-up prompts. These results suggest that approaches similar to {ChatExtract}, due to their simplicity, transferability, and accuracy are likely to become powerful tools for data extraction in the near future. Finally, databases for critical cooling rates of metallic glasses and yield strengths of high entropy alloys are developed using {ChatExtract}.},
	number = {{arXiv}:2303.05352},
	publisher = {{arXiv}},
	author = {Polak, Maciej P. and Morgan, Dane},
	urldate = {2024-01-28},
	date = {2023-06-27},
	eprinttype = {arxiv},
	eprint = {2303.05352 [cond-mat]},
	keywords = {Computer Science - Computation and Language, Lido, Condensed Matter - Materials Science},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/48PNTFN4/Polak e Morgan - 2023 - Extracting Accurate Materials Data from Research P.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/V737D9Q2/2303.html:text/html},
}

@article{gupta_matscibert_2022,
	title = {{MatSciBERT}: A materials domain language model for text mining and information extraction},
	volume = {8},
	rights = {2022 The Author(s)},
	issn = {2057-3960},
	url = {https://www.nature.com/articles/s41524-022-00784-w},
	doi = {10.1038/s41524-022-00784-w},
	shorttitle = {{MatSciBERT}},
	abstract = {A large amount of materials science knowledge is generated and stored as text published in peer-reviewed scientific literature. While recent developments in natural language processing, such as Bidirectional Encoder Representations from Transformers ({BERT}) models, provide promising information extraction tools, these models may yield suboptimal results when applied on materials domain since they are not trained in materials science specific notations and jargons. Here, we present a materials-aware language model, namely, {MatSciBERT}, trained on a large corpus of peer-reviewed materials science publications. We show that {MatSciBERT} outperforms {SciBERT}, a language model trained on science corpus, and establish state-of-the-art results on three downstream tasks, named entity recognition, relation classification, and abstract classification. We make the pre-trained weights of {MatSciBERT} publicly accessible for accelerated materials discovery and information extraction from materials science texts.},
	pages = {1--11},
	number = {1},
	journaltitle = {npj Computational Materials},
	shortjournal = {npj Comput Mater},
	author = {Gupta, Tanishq and Zaki, Mohd and Krishnan, N. M. Anoop and Mausam},
	urldate = {2024-01-28},
	date = {2022-05-03},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Lido, Materials science, Condensed-matter physics},
	file = {Full Text PDF:/home/breno/Zotero/storage/U4YKEWVV/Gupta et al. - 2022 - MatSciBERT A materials domain language model for .pdf:application/pdf},
}

@inproceedings{ponsard_paperquest_2016,
	location = {New York, {NY}, {USA}},
	title = {{PaperQuest}: A Visualization Tool to Support Literature Review},
	isbn = {978-1-4503-4082-3},
	url = {https://doi.org/10.1145/2851581.2892334},
	doi = {10.1145/2851581.2892334},
	series = {{CHI} {EA} '16},
	shorttitle = {{PaperQuest}},
	abstract = {The literature review is a key component of academic research, which allows researchers to build upon each other's work. While modern search engines enable fast access to publications, there is a lack of support for filtering out the vast majority of papers that are irrelevant to the current research focus. We present {PaperQuest}, a visualization tool that supports efficient reading decisions, by only displaying the information useful at a given step of the review. We propose an algorithm to find and sort papers that are likely to be relevant to users, based on the papers they have already expressed interest in and the number of citations. The current implementation uses papers from the {CHI}, {UIST}, and {VIS} conferences, and citation counts from Google Scholar, but is easily extensible to other domains of the literature.},
	pages = {2264--2271},
	booktitle = {Proceedings of the 2016 {CHI} Conference Extended Abstracts on Human Factors in Computing Systems},
	publisher = {Association for Computing Machinery},
	author = {Ponsard, Antoine and Escalona, Francisco and Munzner, Tamara},
	urldate = {2024-01-28},
	date = {2016-05-07},
	keywords = {Para Leitura, information visualization, literature review, sensemaking},
	file = {Ponsard et al. - 2016 - PaperQuest A Visualization Tool to Support Litera.pdf:/home/breno/Zotero/storage/2IWBVMDZ/Ponsard et al. - 2016 - PaperQuest A Visualization Tool to Support Litera.pdf:application/pdf},
}

@article{zhang_survey_2018,
	title = {A survey on visualization for scientific literature topics},
	volume = {21},
	issn = {1343-8875, 1875-8975},
	url = {http://link.springer.com/10.1007/s12650-017-0462-2},
	doi = {10.1007/s12650-017-0462-2},
	abstract = {The topics in scientiﬁc literature illustrate the contents of science domain, and the evolution of topics help in recognizing the research trend and front. Since the number of scientiﬁc works is growing exponentially, it is a great challenge for people to discover new research topics and topic changes. Fortunately, aided by text mining, visualization technologies are being widely used for topic analysis. Visualization is an effective tool for revealing the current status and topic evolution trend in a research ﬁeld. Owing to the importance of topic analysis as well as the lack of a comprehensive description of this theme, we present a survey on the visualization methods for scientiﬁc literature topics. This paper introduces the basic concepts of bibliometrics and the pipeline of topic visualization. Based on the topic analysis tasks, we classify these papers into three categories: topic contents, topic relation, and topic evolution. Furthermore, each part is divided into smaller categories on the basis of the visual patterns. Some existing free software that integrates multiple functions are also introduced. Finally, we discuss the challenges and opportunities in the ﬁeld of topic visualization.},
	pages = {321--335},
	number = {2},
	journaltitle = {Journal of Visualization},
	shortjournal = {J Vis},
	author = {Zhang, Changhong and Li, Zeyu and Zhang, Jiawan},
	urldate = {2024-01-28},
	date = {2018-04},
	langid = {english},
	keywords = {Para Leitura},
	file = {Zhang et al. - 2018 - A survey on visualization for scientific literatur.pdf:/home/breno/Zotero/storage/H85QSA2Y/Zhang et al. - 2018 - A survey on visualization for scientific literatur.pdf:application/pdf},
}

@article{changhong_survey_2017,
	title = {A survey on visualization for scientific literature topics},
	volume = {21},
	doi = {10.1007/s12650-017-0462-2},
	abstract = {The topics in scientific literature illustrate the contents of science domain, and the evolution of topics help in recognizing the research trend and front. Since the number of scientific works is growing exponentially, it is a great challenge for people to discover new research topics and topic changes. Fortunately, aided by text mining, visualization technologies are being widely used for topic analysis. Visualization is an effective tool for revealing the current status and topic evolution trend in a research field. Owing to the importance of topic analysis as well as the lack of a comprehensive description of this theme, we present a survey on the visualization methods for scientific literature topics. This paper introduces the basic concepts of bibliometrics and the pipeline of topic visualization. Based on the topic analysis tasks, we classify these papers into three categories: topic contents, topic relation, and topic evolution. Furthermore, each part is divided into smaller categories on the basis of the visual patterns. Some existing free software that integrates multiple functions are also introduced. Finally, we discuss the challenges and opportunities in the field of topic visualization.

Graphical Abstract
Open image in new window},
	journaltitle = {Journal of Visualization},
	shortjournal = {Journal of Visualization},
	author = {Changhong, Zhang and Li, Zeyu and Zhang, Jiawan},
	date = {2017-12-07},
	keywords = {Para Leitura},
	file = {Full Text PDF:/home/breno/Zotero/storage/L2BJSMEE/Changhong et al. - 2017 - A survey on visualization for scientific literatur.pdf:application/pdf},
}

@inproceedings{peltonen_topic-relevance_2017,
	location = {New York, {NY}, {USA}},
	title = {Topic-Relevance Map: Visualization for Improving Search Result Comprehension},
	isbn = {978-1-4503-4348-0},
	url = {https://dl.acm.org/doi/10.1145/3025171.3025223},
	doi = {10.1145/3025171.3025223},
	series = {{IUI} '17},
	shorttitle = {Topic-Relevance Map},
	abstract = {We introduce topic-relevance map, an interactive search result visualization that assists rapid information comprehension across a large ranked set of results. The topic-relevance map visualizes a topical overview of the search result space as keywords with respect to two essential information retrieval measures: relevance and topical similarity. Non-linear dimensionality reduction is used to embed high-dimensional keyword representations of search result data into angles on a radial layout. Relevance of keywords is estimated by a ranking method and visualized as radiuses on the radial layout. As a result, similar keywords are modeled by nearby points, dissimilar keywords are modeled by distant points, more relevant keywords are closer to the center of the radial display, and less relevant keywords are distant from the center of the radial display. We evaluated the effect of the topic-relevance map in a search result comprehension task where 24 participants were summarizing search results and produced a conceptualization of the result space. The results show that topic-relevance map significantly improves participants' comprehension capability compared to a conventional ranked list presentation.},
	pages = {611--622},
	booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
	publisher = {Association for Computing Machinery},
	author = {Peltonen, Jaakko and Belorustceva, Kseniia and Ruotsalo, Tuukka},
	urldate = {2024-01-28},
	date = {2017-03-07},
	keywords = {Para Leitura, dimensionality reduction, exploratory search, sense-making, visualization},
	file = {Full Text PDF:/home/breno/Zotero/storage/88M3SPJH/Peltonen et al. - 2017 - Topic-Relevance Map Visualization for Improving S.pdf:application/pdf},
}

@article{federico_survey_2017,
	title = {A Survey on Visual Approaches for Analyzing Scientific Literature and Patents},
	volume = {23},
	issn = {1941-0506},
	url = {https://ieeexplore.ieee.org/document/7570239},
	doi = {10.1109/TVCG.2016.2610422},
	abstract = {The increasingly large number of available writings describing technical and scientific progress, calls for advanced analytic tools for their efficient analysis. This is true for many application scenarios in science and industry and for different types of writings, comprising patents and scientific articles. Despite important differences between patents and scientific articles, both have a variety of common characteristics that lead to similar search and analysis tasks. However, the analysis and visualization of these documents is not a trivial task due to the complexity of the documents as well as the large number of possible relations between their multivariate attributes. In this survey, we review interactive analysis and visualization approaches of patents and scientific articles, ranging from exploration tools to sophisticated mining methods. In a bottom-up approach, we categorize them according to two aspects: (a) data type (text, citations, authors, metadata, and combinations thereof), and (b) task (finding and comparing single entities, seeking elementary relations, finding complex patterns, and in particular temporal patterns, and investigating connections between multiple behaviours). Finally, we identify challenges and research directions in this area that ask for future investigations.},
	pages = {2179--2198},
	number = {9},
	journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Federico, Paolo and Heimerl, Florian and Koch, Steffen and Miksch, Silvia},
	urldate = {2024-01-28},
	date = {2017-09},
	note = {Conference Name: {IEEE} Transactions on Visualization and Computer Graphics},
	keywords = {Para Leitura, Data visualization, documents, Law, Metadata, patents, Patents, scientific literature, survey, Visualization, Writing},
	file = {IEEE Xplore Abstract Record:/home/breno/Zotero/storage/LREMUGQ5/7570239.html:text/html;IEEE Xplore Full Text PDF:/home/breno/Zotero/storage/WYLKEZBH/Federico et al. - 2017 - A Survey on Visual Approaches for Analyzing Scient.pdf:application/pdf},
}

@article{van_eck_citnetexplorer_2014,
	title = {{CitNetExplorer}: A new software tool for analyzing and visualizing citation networks},
	volume = {8},
	issn = {17511577},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1751157714000662},
	doi = {10.1016/j.joi.2014.07.006},
	shorttitle = {{CitNetExplorer}},
	pages = {802--823},
	number = {4},
	journaltitle = {Journal of Informetrics},
	shortjournal = {Journal of Informetrics},
	author = {Van Eck, Nees Jan and Waltman, Ludo},
	urldate = {2024-01-28},
	date = {2014-10},
	langid = {english},
	keywords = {Para Leitura},
	file = {Van Eck e Waltman - 2014 - CitNetExplorer A new software tool for analyzing .pdf:/home/breno/Zotero/storage/BJNLNV6E/Van Eck e Waltman - 2014 - CitNetExplorer A new software tool for analyzing .pdf:application/pdf},
}

@misc{van_eck_text_2011,
	title = {Text mining and visualization using {VOSviewer}},
	url = {http://arxiv.org/abs/1109.2058},
	abstract = {{VOSviewer} is a computer program for creating, visualizing, and exploring bibliometric maps of science. In this report, the new text mining functionality of {VOSviewer} is presented. A number of examples are given of applications in which {VOSviewer} is used for analyzing large amounts of text data.},
	number = {{arXiv}:1109.2058},
	publisher = {{arXiv}},
	author = {van Eck, Nees Jan and Waltman, Ludo},
	urldate = {2024-01-28},
	date = {2011-09-09},
	eprinttype = {arxiv},
	eprint = {1109.2058 [cs]},
	keywords = {Para Leitura, Computer Science - Digital Libraries},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/V8TY9MJ9/van Eck e Waltman - 2011 - Text mining and visualization using VOSviewer.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/H8XFMBSK/1109.html:text/html},
}

@misc{beltagy_scibert_2019,
	title = {{SciBERT}: A Pretrained Language Model for Scientific Text},
	url = {http://arxiv.org/abs/1903.10676},
	shorttitle = {{SciBERT}},
	abstract = {Obtaining large-scale annotated data for {NLP} tasks in the scientific domain is challenging and expensive. We release {SciBERT}, a pretrained language model based on {BERT} (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. {SciBERT} leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific {NLP} tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over {BERT} and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
	number = {{arXiv}:1903.10676},
	publisher = {{arXiv}},
	author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
	urldate = {2024-01-26},
	date = {2019-09-10},
	eprinttype = {arxiv},
	eprint = {1903.10676 [cs]},
	keywords = {Computer Science - Computation and Language, Lido},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/GGPFJN4H/Beltagy et al. - 2019 - SciBERT A Pretrained Language Model for Scientifi.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/LIA9IB35/1903.html:text/html},
}

@misc{lee_interactive_2016,
	title = {An Interactive Machine Learning Framework},
	url = {http://arxiv.org/abs/1610.05463},
	abstract = {Machine learning ({ML}) is believed to be an effective and efficient tool to build reliable prediction model or extract useful structure from an avalanche of data. However, {ML} is also criticized by its difficulty in interpretation and complicated parameter tuning. In contrast, visualization is able to well organize and visually encode the entangled information in data and guild audiences to simpler perceptual inferences and analytic thinking. But large scale and high dimensional data will usually lead to the failure of many visualization methods. In this paper, we close a loop between {ML} and visualization via interaction between {ML} algorithm and users, so machine intelligence and human intelligence can cooperate and improve each other in a mutually rewarding way. In particular, we propose "transparent boosting tree ({TBT})", which visualizes both the model structure and prediction statistics of each step in the learning process of gradient boosting tree to user, and involves user's feedback operations to trees into the learning process. In {TBT}, {ML} is in charge of updating weights in learning model and filtering information shown to user from the big data, while visualization is in charge of providing a visual understanding of {ML} model to facilitate user exploration. It combines the advantages of both {ML} in big data statistics and human in decision making based on domain knowledge. We develop a user friendly interface for this novel learning method, and apply it to two datasets collected from real applications. Our study shows that making {ML} transparent by using interactive visualization can significantly improve the exploration of {ML} algorithms, give rise to novel insights of {ML} models, and integrates both machine and human intelligence.},
	number = {{arXiv}:1610.05463},
	publisher = {{arXiv}},
	author = {Lee, Teng and Johnson, James and Cheng, Steve},
	urldate = {2024-01-26},
	date = {2016-10-18},
	eprinttype = {arxiv},
	eprint = {1610.05463 [cs]},
	keywords = {Computer Science - Machine Learning, Para Leitura, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/YY48BFQ8/Lee et al. - 2016 - An Interactive Machine Learning Framework.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/S7FUNMTY/1610.html:text/html},
}

@misc{wang_wizmap_2023,
	title = {{WizMap}: Scalable Interactive Visualization for Exploring Large Machine Learning Embeddings},
	url = {http://arxiv.org/abs/2306.09328},
	shorttitle = {{WizMap}},
	abstract = {Machine learning models often learn latent embedding representations that capture the domain semantics of their training data. These embedding representations are valuable for interpreting trained models, building new models, and analyzing new datasets. However, interpreting and using embeddings can be challenging due to their opaqueness, high dimensionality, and the large size of modern datasets. To tackle these challenges, we present {WizMap}, an interactive visualization tool to help researchers and practitioners easily explore large embeddings. With a novel multi-resolution embedding summarization method and a familiar map-like interaction design, {WizMap} enables users to navigate and interpret embedding spaces with ease. Leveraging modern web technologies such as {WebGL} and Web Workers, {WizMap} scales to millions of embedding points directly in users' web browsers and computational notebooks without the need for dedicated backend servers. {WizMap} is open-source and available at the following public demo link: https://poloclub.github.io/wizmap.},
	number = {{arXiv}:2306.09328},
	publisher = {{arXiv}},
	author = {Wang, Zijie J. and Hohman, Fred and Chau, Duen Horng},
	urldate = {2024-01-26},
	date = {2023-06-15},
	eprinttype = {arxiv},
	eprint = {2306.09328 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Human-Computer Interaction, Lido, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/TAAVHX52/Wang et al. - 2023 - WizMap Scalable Interactive Visualization for Exp.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/6Y3FHRL7/2306.html:text/html},
}

@misc{lin_vector_2023,
	title = {Vector Search with {OpenAI} Embeddings: Lucene Is All You Need},
	url = {http://arxiv.org/abs/2308.14963},
	doi = {10.48550/arXiv.2308.14963},
	shorttitle = {Vector Search with {OpenAI} Embeddings},
	abstract = {We provide a reproducible, end-to-end demonstration of vector search with {OpenAI} embeddings using Lucene on the popular {MS} {MARCO} passage ranking test collection. The main goal of our work is to challenge the prevailing narrative that a dedicated vector store is necessary to take advantage of recent advances in deep neural networks as applied to search. Quite the contrary, we show that hierarchical navigable small-world network ({HNSW}) indexes in Lucene are adequate to provide vector search capabilities in a standard bi-encoder architecture. This suggests that, from a simple cost-benefit analysis, there does not appear to be a compelling reason to introduce a dedicated vector store into a modern "{AI} stack" for search, since such applications have already received substantial investments in existing, widely deployed infrastructure.},
	number = {{arXiv}:2308.14963},
	publisher = {{arXiv}},
	author = {Lin, Jimmy and Pradeep, Ronak and Teofili, Tommaso and Xian, Jasper},
	urldate = {2024-09-18},
	date = {2023-08-28},
	eprinttype = {arxiv},
	eprint = {2308.14963 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/V9HVDN4I/Lin et al. - 2023 - Vector Search with OpenAI Embeddings Lucene Is Al.pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/8VCS3XEX/2308.html:text/html},
}

@misc{gao_retrieval-augmented_2024,
	title = {Retrieval-Augmented Generation for Large Language Models: A Survey},
	url = {http://arxiv.org/abs/2312.10997},
	doi = {10.48550/arXiv.2312.10997},
	shorttitle = {Retrieval-Augmented Generation for Large Language Models},
	abstract = {Large Language Models ({LLMs}) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation ({RAG}) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. {RAG} synergistically merges {LLMs}' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of {RAG} paradigms, encompassing the Naive {RAG}, the Advanced {RAG}, and the Modular {RAG}. It meticulously scrutinizes the tripartite foundation of {RAG} frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in {RAG} systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
	number = {{arXiv}:2312.10997},
	publisher = {{arXiv}},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	urldate = {2024-09-18},
	date = {2024-03-27},
	eprinttype = {arxiv},
	eprint = {2312.10997 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/breno/Zotero/storage/N62KMQYM/Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language .pdf:application/pdf;arXiv.org Snapshot:/home/breno/Zotero/storage/PV56WQKP/2312.html:text/html},
}
